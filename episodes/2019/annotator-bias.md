In this episode, we speak with [Mor Geva](https://mega002.github.io/), lead author of the recent paper: [Are We Modeling the Task or the Annotator? An Investigation of Annotator Bias in Natural Language Understanding Datasets](https://arxiv.org/abs/1908.07898), which explores some unintended consequences of the typical procedure followed for generating corpora.

Mor began with a background into what building question-answering machine learning is about and the problems in this domain. To build robust models, she spoke about the need to access datasets. She also discussed how the data gathering and data annotation process is done. She revealed the three datasets she used for her study: The [CommonsenseQA dataset](https://www.tau-nlp.sites.tau.ac.il/commonsenseqa), the [OpenBookQA dataset](https://allenai.org/data/open-book-qa), and the [MultiNLI](https://www.nyu.edu/projects/bowman/multinli/) dataset. She talked about the problem of bias that may arise during annotation.

Mor then delved deeper into the methodology for building the model and how she tested for annotator-based bias. She also discussed how the annotator bias can lead to poor generalization of the model.

Mor spoke about the other possibilities for solving the annotator bias problem during data collection. She specifically spoke about the use of consensus during data collection. Additionally, she discussed the possibility of the language model capturing the bias during training and be robust to them.

Rounding up, she gave her opinion on the popularity of large models such as BERT and whether we are on the right track for AGI. You can stay in touch with Morâ€™s research from her [website](https://mega002.github.io/) or follow her on Twitter [@megamor2](https://twitter.com/megamor2).
