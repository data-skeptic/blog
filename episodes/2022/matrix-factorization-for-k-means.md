# Matrix Factorization for K-means

Sybille is an assistant professor in the Data Mining group at TU Eindhoven in the Netherlands. Her research includes work with Matrix Factorization, particularly with clustering objectives, and exploring the relationship between this methodology to  Deep Learning.

  

Sybille started off by explaining from a high-level, how matrix factorization is related to deep learning, using autoencoders as an example. She also stated the difference in terms of similarity initialization in both matrix factorization methods and deep learning methods. In a bid to clear the statement that softmax formula and K-means clustering have the same working principle, Sybille detailed how softmax works. 

  

She also discussed what an embedded space looks like and how the classes are distributed using cones. While spectral clustering does not have clear centroids as in the case of K-means clustering, it can be seen as a special kind of K-means called [spherical K-means clustering](https://www.jstatsoft.org/article/download/v050i10/633#:~:text=Spherical%20k%2Dmeans%20clustering%20is,representa%2D%20tions%20of%20the%20documents.). 

Sybille further explained the practical implications of spectral clustering mathematical deductions such as how clustering occurs in the embedded space and how to determine the right confidence for each class. But you may ask. Is spectral clustering robust to deceptive inputs during training? Sybille discussed how the model works in adversarial attacks and learning.

  

She also discussed a problem she faced during training - having the model not to return probabilities. If you are wondering how this was a problem, Sybille talked about it. She additionally spelled out the other challenges she faced during implementation and she found her way around them. 

  

Sybille further discussed her results and how the result compares with the conventional neural networks. She went on to explain how her methods edge conventional neural networks in terms of classification accuracy, confidence levels, and robustness to adversarial attacks. 

One would think with better results, there would be mass migration from the traditional neural networks. Well, Sybille talked about these possibilities and the bottlenecks users may face with its results.  

  
In conclusion, Sybille had her thoughts on further research in this field and what she is most excited about. You can follow Sybille on Twitter [@lsebyl](https://twitter.com/Ilsebyl). You can also check her [website](https://sibylse.github.io/ZeroShades/pubs/) where she plans to publish tutorials on Matrix Factorization with binary constraints.
