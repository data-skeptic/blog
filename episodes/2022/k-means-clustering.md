## K-Means Clustering

k-means clustering.

Allow me to sum this topic up in 6 words.

Partition n observations into k clusters.

Imagine a dataset consisting of these $n=9$ notes.

And I want to group these notes into two different clusters: $A$ and $B$.

In order to indicate which cluster a note is in or not in, I'm going labeling them with different timbres.

Notes in $A$ sound like this.

Notes in $B$ sound like this.

I could cluster our datalike this, but that seems imbalanced.

I could cluster our datalike this, but that probably feels off to the intuition of most listeners.

Clustering these notes into two groups probably sounds most pleasing to the ears in this configuration.

But why should we prefer one clustering result over another?

And what about the number of clusters?  I was using $k=2$, but doesn't $k=3$ sound like it's clustered better for our notes?  Why?

As with most questions, the answer lies in the data.

The notes themselves are the data and in this regard, our data is one dimensional.

A single parameter, the frequency, is sufficient to fully express each observation in our dataset.

So we have an array of numbers.  In this case the frequencies in hertz are 100 200 300 a b c and blah blah blah.

Like I said, one dimensional, but we can scale this idea up in side, let's go 2-dimensional.

Picture before you a blank wall freshly painted flawlessly eggshell white.

In walk three people with paintball guns who proceed to spray the wall in a pattern that looks a bit like the sillouette of mickey mouse's head.  But instead of producing 3 solid circle, our paintballs more stray at the center of their 3 respective targets, giving us two dimensional gaussian distributions.  More paintballs hit near the center of each gunman's target but with somes variance, they stray from the center point.

Nothing is more perfect for analysis then gaussian blobs of data.  If your dataset fits this format, you're in luck.  Algorithms like k-means are built for data like this.

Ok, repaint the wall white again.  Let's reset this.

Our paintballers come back in but this time, it's two gaussian blobs at the top, and the third paintballer completes the image of a smilely face with a swish of paintballs along the bottom.

Picture this painting on the canvas of your imagination.  Now each of us has imagined a slightly different paintball smiley face.

Mine has a lot of variance in it because I know paintball guns aren't really precision instruments.  In fact the boundaries of the three groups are a little hard to define.  I know the data was generated by three independent paintballers, but if they all use the same color, we have to look purely at the data.  There are likely to be a lot of paint hits on the borders that don't fit into any intuitive notion of the 3 intended clusters of the smile face.

If we think of the two eyes as two clusters, we can take the average of all the points in that cluster and we should find the exact middle of the eye.  That's promising.  But if we end up clustering all the smile points together, picture the average of that cluster.  It's somewhere in the middle of the arc not close to the actual points in the cluster.  That's a bit odd.  Our results will certainly be sensitive to the data we use.

If your assignment is to create a clustering algorithm that maps n data points to k clusters, you're forced to make a decision about each point.  For k-means clustering algorithms, that's done based on the distance between points in the cluster to the center of the assigned cluster.  In the case of a cluster of notes, the centroid is their average frequency.  In the case of paintballs it would be the average x and y coordinates on the wall.

Your data can have any number of dimensions.  Your dimensions could be dummy variables or other kinds of features.  But just as we saw some ambiguity in how to cluster the smiley face, you should apply a good amount of skepticism to what your clustering results really mean.  Algorithms like k-means always give you a result, even if your data is random coin tosses or lottery numbers.  It's up to you to determine if the clustering result is informative or not.

There's a lot more to be cautious about which we'll revisit later in this series.  For now, don't panic.  Let's explore the algorithm before we cast our skeptical eye on it's results.  For now, assume you've got textbook data consisting of well separable gaussian blobs.

So on to the algorithm.  When just about anyone talks about k-means clustering, they mean using Lloyd's algorithm.  There's alternatives and improvements on this classic algorithm, some of which we'll discuss on upcoming episodes.  But for now, let's step through the plain vanilla Lloyd's algorithm.

It comes in three steps

Initialize

Assign

Recalculate

The initialize phase is actually rather trivial.  For whatever $k$ we're going to use, let's say $k=3$, you randomly select three points and place your initial centroids at these random locations.  It's very much like throwing 3 darts at the wall.  I mean, you can't do much worse than randomly assigning k locations, surely there's some more intelligent way to initialize this problem, right?  A popular approach is called k-means++ which we'll cover in the near future.

For now, take whatever $k$ you're given and randomly set the presumptive centroids for your $k$ clusters.  Once done, consider yourself initialized.  You've placed your initial guess for where the center of each cluster or centroid might be found.

Let's now take a look at steps 2 and 3: the assign and recalculate steps.

During assignment, you scan through every data point (paint balls in reverse)

Compare each data point to each of your centroids.  Regardless of any previous cluster assignment, assign that data point to the nearest centroid.

Once you've assigned every point to a centroid, step 2 is complete.  We move on to step 3 where we recalculate the centroids.

Given the new assignments, the centroid's current location probably isn't at the exact center of all it's cluster members.  So average them and move the centroid to that new location.

So we're done, but we're not done.  We have centroids.  Every point has a cluster assignment.  But remember that we moved the centroids to a new location.  That means there's a chance some data point is now going to switch sides because they're now closer to the centroid's new location.  So we go back to step 2 and repeat the reassignment.  Then repeat the recalculation of the centroids.

And it's actually possible to get caught in an infinite loop here.  You need to establish some sort of halting criteria.  An obvious one is to stop if a particular iteration doesn't result in any changes of cluster assignments.  But more commonly, people stop on number of iterations or via an optimality criteria which is to say we're confident we've gotten epsilon close to the right answer.

Let's get a bit more technical now.

k-means clustering is the poster child for unsupervised learning.  We use the word unsupervised to contrast with supervised learning in which you have training data that is labelled and the objective is to learn a function which can predict the true label of unlabelled examples.  But like many things in life, that sounds a lot more binary than it actually is.  You can bucket algorithms like logistic regression and random forest into the same bin because they're trying to solve the same problem just with different methods.  Unsupervised learning, on the other hand, is more of a grabbag for unrelated machine learning tasks linked together only by their common property that they don't require labelled data.    In terms of commonality, unsupervised learning is like my club that you can only join if your first name starts with k.

Lloyd's algorithm requires one parameter, the famous $k$.  If your dataset has $N$ elements, there are $k^N$ possible labellings you can output.  As your dataset grows, the number of options grows exponentially.  Yikes!  There's no way we can check every possible combination of labellings to find the best one if that $N$ gets too big.  This is a classic search optimization problem.

Optimziation problems involve finding the best answer you can out of all possible answers.  In our case, "best" needs a numeric definition.  So here it goes.

Remember our data from earlier.  We asked for $k=3$ clusters.  And we preferred this... to this...

In the losing example, remember the centroid is the average of all members, our centroids end up pretty close together under this assignment.  Which means they can be pretty distant from some of the members.

The winning example wins our favor because it minimizes the distance from each point to it's associated centroid.

Our distance metric is a function that effectively scores each solution, and we need an algorithm that's going to find it's way to the best score, for this optimization problem, the best score is the one that minimizes the average distance between your data and it's associated centroids.

Remember we've got $k^N$ possible labelings, as the size of your data gets larger, it gets exponentially harder to do an exhaustive search.  Instead we treat it like an optimization problem, searching for the optimal solution.  However, that search comes with no real guarantees.  Our search may get trapped in some local optima if we start with an unlucky initialization.  Solving the general case of this problem is NP-Hard.  If you don't know what that means, don't worry, we've got a few good computational theory episodes in the archives that will help but it's not important for this season.  The key take away is that a clustering result is not guaranteed to be the best possible result.  However, in practice, I worry much more about the underlying shape of my data and how suitable it is for k-mens more than I worry about how epsilon close I might be to perfection.

Let's talk about an interesting pitfall.  Imagine you have some data on different automobiles.  You have the year of manufacture, the average miles per gallon, the manufacturers suggested retail price, and a few other statistics about a variety of cars.  For whatever reason you've decided to cluster your car data and you decide to blindly apply k-means clustering.

Let's review our variables.

Year of manufacture.  The dataset might go back to the 70s, maybe.  Up to the present.  In other words numbers ranging from 1970 to 2022 as we record this.

Miles per gallon.  That's an interesting one, because currently, there are gas cars, hybrids, and all electric.  I've had a hybrid for the last 8 years and I get about 40-45 mpg.  For many years before that I didn't own a car, and before that I recall being very happy to get 20 mpg with the the cars I had.  I'm not even sure what value we put for mpg when it comes to electric cars, so let's just ignore that.

I also mentioned retail price.  Some cars sell used for a few hundred dollars.  Less often, cars can sell for hundreds of thousands of dollars.  But for a moment think about these ranges.  Year of manufacture - roughly a 50 year range.  Miles per gallon - even smaller variance.  Retail price - pretty large variance if we measure it in dollars.

Units matter.  If you blindly ask a machine to solve an optimization problem on data like this, it will exploit the unnormalized nature of your data.  If the x-axis is in years, and the y-axis is in mpg, and the z-axis is in dollars, then the biggest distances between data points are going to be on the z-axis since price has the largest range of the 3 variables.  A native implementation of k-means clustering would yield automobile data clustered primarily on price while ignoring the other variables.

Hey, it's unsupervised learning.  Whose to say what's right?  But clearly, something is wrong if we do this blindly.  And that's why most software implementations of k-means clustering will automatically apply normalize your data in a standard statistical so that the units can't bias your results so significantly.  In truth, you probably don't need to worry about this fact since its numerically handled under the hood.  However, if your clustering results are going to inform an important decision you need to make, it might be worth a deep dive here.

Another variable you'll need to consider is the size of your data.  The vast majority of listeners will never encounter the scale of dataset that requires you to take special care.  For most of you, your interactions with k-means are likely to fit snuggly in memory such that you can use practical tools like sk-learn to do a one-time process of clustering your data.  Even if that process had to run overnight to complete, so what?  Get your results monday morning when you get back to your desk.  On small datasets processed with modern compute, your clustering results are going to available pretty quickly.

I can't imagine what urgent decision might need to be informed by the results of our clustering effort.  So in practice, and informed only by my limited anacdotal life experience, I've found that in cases where k-means clustering is useful, it's never a stand alone project.  I've found no intrinsic virtue in arbitrarily clustering data.  Again, in my own finite experience, k-means is one step in a bigger recipe, typically as a feature engineering step.  

Given some input data, you can apply lloyd's algorithm or some variant to develop an efficient classifer that can label my underlying dataset in some meaningful way.  Every time I've used k-means beyond classroom level analysis, it's been in pursuit of pulling a new label out of thin air that I can apply to some data.  Should I trust that label?  No.  Always be appropriately skeptical of your input data.  But if your data does exhibit some modal property, and k-means is able to recognize it, then the clusters each of your data points gets assigned to might be a novel insight that you get, seemingly for free, from this unsupervised algorithm, which can be passed on to another process, typically some supervised learning which can use the cluster id as a feature and let the data speak for itself about how useful that is.

To wind up, let's talk about two remaining topics I want to cover related to k-means, the ebow method and silouette scores.

So we've got the $k$ in k-means that we need to provide as input.  What's the best $k$?  There's not a provably best solution here.  It's more about trade offs.  As you use more clusters, those clusters obviously fit the data better, but at some point they undoubtable begin to over-fit the data.  To make a decision like this, most people will recommend the elbow method in which you create a plot with $k$ on the x axis and some metric on the y-axis, typically the explained variance, which measures how well the clusters fit the data.  With $k=N$, every data point lives in it's own cluster.  It's perfectly overfit.  There's no variance in the groups, so 100% of variance is explained.  The curve from $k=1$ to $k=N$ will be increasing and the hope is that this visualization will demonstrate an elbow in the curve - somewhere that is a sort of obvious point of inflection suggesting which $k$ can give you the best bang for your buck without being too large.  In practice, my personal experience is that this is a very subjective process.  Your mileage may vary.

Silhouette scores, on the other hand, I really like.  Its a deterministic single value that's painless to compute and gives me a nice quantitative signal.  It's based on two variables.  First, the average distance all the points in a cluster have from their associated centroid. Naturally we want that to be small. Second, the average distance from each point to the other centroids.  That one should be large.  A little arithmetic and we've got ourselves a number between -1 and 1 that hints at the quality of the clusters we've calcuated.


# Later

building with k-means: image segmentation, speaker detection
Gaussian Mixture Model
Mini-batch k-means


