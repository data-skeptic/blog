# Big Bench
Today, we are joined by Mor Geva, a researcher at the Allen Institute for AI, with a focus area in the interpretability and robustness of language models. Mor is a previous guest on the show. The last time, she spoke about [annotator bias](https://dataskeptic.com/blog/episodes/2019/annotator-bias) in language models and how it affects the robustness of NLP models. Today, she follows up on that study, [investigating where bias starts](https://arxiv.org/abs/2205.00415).

She started by discussing a pattern she observed with datasets from crowdfunded workers. This was largely due to the instructions given to annotators by the researcher. She then detailed ways researchers can frame questions/instructions to avoid propagating bias when hiring crowdfunded workers.

Mor spoke about the [StrategyQA dataset](https://allenai.org/data/strategyqa), the dataset on which the language model was trained. She discussed how the data was gathered and the steps taken to ensure the data was diversified in terms of topics and reasoning types.

Mor then discussed how well the trained model answered questions from prompts. She spoke about other models that were trained on the StrategyQA dataset and the results they obtained. She highlighted possible reasons the [top-ranking models in the Leaderboard](https://leaderboard.allenai.org/) performed well.

Mor then discussed the place of benchmarks in advancing language models. She particularly spoke about [BigBench](https://github.com/google/BIG-bench), a Google benchmark that measures the capabilities of language models. In closing, she gave her take on whether the trajectory in language models will lead to AGI. She highlighted some limitations with large language models. You can follow Mor on Twitter [@megamor2](https://twitter.com/megamor2) or on her [webpage](https://mega002.github.io/).
