# Cuttlefish Model Tuning

Hongyi Wang, a Senior Researcher at the Machine Learning Department at Carnegie Mellon University, joins us. His research is in the intersection of systems and machine learning. He discussed his research paper, [Cuttlefish: Low-Rank Model Training without All the Tuning](https://arxiv.org/abs/2305.02538), on todayâ€™s show.

Hogyi started by sharing his thoughts on whether developers need to learn how to fine-tune models.  He then spoke about the need to optimize the training of ML models, especially as these models grow bigger. He discussed how data centers have the hardware to train these large models but not the community. He then spoke about the Low-Rank Adaptation (LoRa) technique and where it is used.

Hongyi discussed the Cuttlefish model and how it edges LoRa. He shared the use cases of Cattlefish and who should use it. Rounding up, he gave his advice on how people can get into the machine learning field. He also shared his future research ideas.

Learn more about Hongyi on his [homepage](https://hwang595.github.io/) or Twitter [@HongyiWang10](https://twitter.com/hongyiwang10?lang=en).