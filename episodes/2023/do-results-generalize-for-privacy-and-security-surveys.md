# Do Results Generalize for Privacy and Security Surveys

On the show today, we are joined by Jenny Tang, a Ph.D. student of societal computing at Carnegie Mellon University. She is also affiliated with Skylab, the security and privacy institute of the university. She joins us to discuss her study that assessed the use of online surveys by privacy and security researchers.

Jenny began by discussing why surveys are important to get a generalized perspective. She also explained how your population can differ depending on the goal of the research. She expatiated on the two kinds of population — target population and sample population. 

Jenny spoke about the two widely used online platforms for conducting surveys in her field. She also discussed some drawbacks of using such online platforms. Jenny shared how she and her coauthors used a [probabilistic sample from Pew Research Center](https://www.pewresearch.org/internet/dataset/american-trends-panel-wave-49/) as a baseline for ground truth comparison. She also shared examples of security and privacy questions asked in her surveys.

Jenny spoke about the three other samples used in comparison with the Pew sample: the Amazon Mechanical Turk (MTurk) sample, the Prolific gender balance sample, and the Prolific representative sample. She discussed the survey results across these samples and concluded that the MTurk sample produced an outlier result. Jenny explained the possible reasons for this.

Jenny then discussed the use of attention checks to clean the data from MTurk and later achieved results similar to the other samples. She shared lessons learned and how other researchers can achieve more generalizable results from their surveys.

Rounding up, Jenny gave her take on people’s level of awareness to online security and privacy matters. She revealed some of her future research work on academic misinformation, bridging the gap between policy and technical communities. You can follow Jenny on Twitter [@WhatIsCyber](https://twitter.com/WhatIsCyber).